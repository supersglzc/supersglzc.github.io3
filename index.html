<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
    <meta name=viewport content="width=device-width, initial-scale=1">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
    <style type="text/css">
        /* Color scheme stolen from Sergey Karayev */
        
        a {
            color: #1772d0;
            text-decoration: none;
        }
        
        a:focus,
        a:hover {
            color: #f09228;
            text-decoration: none;
        }
        
        body,
        td,
        th,
        tr,
        p,
        a {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px
        }
        
        strong {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
        }
        
        heading {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 24px;
        }
        
        papertitle {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 15px;
            font-weight: 700
        }
        
        name {
            font-family: 'Lato', Verdana, Helvetica, sans-serif;
            font-size: 32px;
        }
        
        .one {
            width: 160px;
            height: 160px;
            position: relative;
        }
        
        .two {
            width: 160px;
            height: 160px;
            position: absolute;
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        
        .fade {
            transition: opacity .2s ease-in-out;
            -moz-transition: opacity .2s ease-in-out;
            -webkit-transition: opacity .2s ease-in-out;
        }
        
        span.highlight {
            background-color: #ffffd0;
        }
    </style>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <!-- https://fontawesome.com/cheatsheet -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.3.1/css/all.css" integrity="sha384-mzrmE5qonljUremFsqc01SB46JvROS7bZs3IO2EmfFsd15uHvIt+Y8vEf7N7fWAU" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css">
    <link rel="icon" type="image/png" href="files/WechatIMG18.jpg">
    <title>Zechu (Steven) Li</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
</head>
<script>
    function toggleblock(blockId) {
        var block = document.getElementById(blockId);
        if (block.style.display == 'none') {
            block.style.display = 'block';
        } else {
            block.style.display = 'none';
        }
    }

    function hideblock(blockId) {
        var block = document.getElementById(blockId);
        block.style.display = 'none';
    }
</script>
<!-- Place this tag in your head or just before your close body tag. -->
<script async defer src="https://buttons.github.io/buttons.js"></script>

<body>
    <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
            <td>
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td width="67%" valign="middle">
                            <p align="center">
                                <name>Zechu (Steven) Li </name>
                            </p>
                            <p>
                                I am currently a research assistant at <a href="https://www.csail.mit.edu/" target="_blank">MIT CSAIL</a>, advised by Prof. <a href="http://people.csail.mit.edu/pulkitag/" target="_blank">Pulkit Agrawal</a>. 
                                My research interest lies in reinforcement learning, especially its high-performance and scalable systems and applications (e.g., robotics, finance, and transportation).
                            </p>
                            <p>
                                Prior to this, I received my bachelor's degree from <a href="https://www.columbia.edu/" target="_blank">Columbia University</a> in May 2022, majoring in <b>computer science</b>.
                                During my undergraduate studies, I was fortunate to work with Prof. <a href="https://www.ee.columbia.edu/~wangx/">Xiaodong Wang</a>, Prof. <a href="https://scholar.google.com.au/citations?user=Q5oC62EAAAAJ&hl=en">Anwar Walid</a> and Prof.
                                <a href="https://sharondi-columbia.wixsite.com/ditectlab">Sharon (Xuan) Di</a>.
                            </p>
                           
                            <p align=center>
                                <a href="mailto:zechu@mit.edu">Email</a> &nbsp/&nbsp
                                <!-- <a href="files/Steven_Li_Resume.pdf" target="_blank">CV</a> &nbsp/&nbsp -->
                                <a href="https://scholar.google.com/citations?user=FI_6by0AAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
                                <a href="https://github.com/supersglzc" target="_blank">GitHub</a>&nbsp/&nbsp
                                <a href="https://www.linkedin.com/in/zechu-li-66a7741b3/" target="_blank">LinkedIn</a>
                            </p>
                        </td>
                        <td width="33%">
                            <img src="files/WechatIMG18.jpg" style="display:block;" width="100%">
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td>
                            <heading>Research Projects</heading> [* Equal contribution]
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="https://arxiv.org/abs/2406.00681" target="_blank">
                                <img src="images/ddiffpg.png" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Learning Multimodal Behaviors from Scratch with Diffusion Policy Gradient
                            </papertitle>
                            <br>
                            <strong>Zechu Li</strong>,
                            Rickmer Krohn,
                            <a href="https://taochenshh.github.io/">Tao Chen</a>,
                            <a href="https://anuragajay.github.io/">Anurag Ajay</a>,
                            <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
                            <a href="https://pearl-lab.com/people/georgia-chalvatzaki/">Georgia Chalvatzaki</a>
                            
                            <br>
                            <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2024
                            <br>
                            <a href="https://arxiv.org/abs/2406.00681" target="_blank">paper</a> /
                            <a href="https://supersglzc.github.io/projects/ddiffpg/" target="_blank">website</a>
                            <p></p>
                            <p>
                                This paper introduces Deep Diffusion Policy Gradient (DDiffPG), a novel actor-critic algorithm that learns multimodal policies as diffusion models from scratch while maintaining versatile behaviors. 
                                DDiffPG employs mode-specific Q-learning to mitigate the RL objective's greediness, ensuring improvement across all modes.
                                Empirical studies demonstrate DDiffPG’s effectiveness in mastering multimodal behaviors in complex, high-dimensional continuous control tasks with sparse rewards and dynamic replanning in maze navigation with unseen obstacles.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="http://arxiv.org/abs/2403.03949" target="_blank">
                                <img src="images/realto.gif" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Reconciling Reality through Simulation: A Real-to-Sim-to-Real Approach for Robust Manipulation
                            </papertitle>
                            <br>
                            <a href="https://marceltorne.github.io/">Marcel Torne</a>,
                            <a href="https://anthonysimeonov.github.io/">Anthony Simeonov</a>,
                            <strong>Zechu Li</strong>,
                            April Chan,
                            <a href="https://taochenshh.github.io/">Tao Chen</a>,
                            <a href="https://abhishekunique.github.io/">Abhishek Gupta*</a>,
                            <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal*</a>
                            <br>
                            <em>Robotics: Science and Systems (RSS)</em>, 2024
                            <br>
                            <a href="http://arxiv.org/abs/2403.03949" target="_blank">paper</a> /
                            <a href="https://real-to-sim-to-real.github.io/RialTo/" target="_blank">website</a> /
                            <a href="https://github.com/real-to-sim-to-real/RialToPolicyLearning" target="_blank">code</a>
                            <p></p>
                            <p>
                                To learn performant, robust policies without the burden of unsafe real-world data collection or extensive human supervision, we propose RialTo, a system for robustifying real-world imitation learning policies via reinforcement learning in "digital twin" simulation environments constructed on the fly from small amounts of real-world data. 
                                To enable this real-to-sim-to-real pipeline, RialTo proposes an easy-to-use interface for quickly scanning and constructing digital twins of real-world environments.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="https://www.mdpi.com/2073-4336/14/3/41" target="_blank">
                                <img src="images/games-14-00041-g001.png" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Social Learning for Sequential Driving Dilemmas
                            </papertitle>
                            <br>
                            Xu Chen,
                            <a href="https://sharondi-columbia.wixsite.com/ditectlab">Sharon (Xuan) Di</a>,
                            <strong>Zechu Li</strong>
                            <br>
                            <em>Games</em>, 2023
                            <br>
                            <a href="https://www.mdpi.com/2073-4336/14/3/41" target="_blank">paper</a>
                            <p></p>
                            <p>
                                Autonomous driving (AV) technology has elicited discussion on social dilemmas where trade-offs between individual preferences, social norms, and collective interests may impact road safety and efficiency. 
                                In this study, we aim to identify whether social dilemmas exist in AVs’ sequential decision making, which we call “sequential driving dilemmas” (SDDs), 
                                to help policymakers and AV manufacturers better understand under what circumstances SDDs arise and how to design rewards that incentivize AVs to avoid SDDs.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="https://openreview.net/pdf?id=vFvw8EzQNLy" target="_blank">
                                <img src="files/parallel_scheme.png" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Parallel Q-Learning: Scaling Off-policy Reinforcement Learning under Massively Parallel Simulation
                            </papertitle>
                            <br>
                            <strong>Zechu Li*</strong>,
                            <a href="https://taochenshh.github.io/">Tao Chen*</a>,
                            <a href="https://williamd4112.github.io/">Zhang-Wei Hong</a>,
                            <a href="https://anuragajay.github.io/">Anurag Ajay</a>,
                            <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>
                            <br>
                            <em>International Conference on Machine Learning (ICML)</em>, 2023
                            <br>
                            <a href="https://openreview.net/pdf?id=vFvw8EzQNLy" target="_blank">paper</a> /
                            <a href="https://github.com/Improbable-AI/pql">code</a>
                            <p></p>
                            <p>
                                We present a novel parallel Q-learning framework that not only gains better sample efficiency but also reduces the training wall-clock time compared to PPO. 
                                Different from prior works on distributed off-policy learning, such as Apex, our framework is designed specifically for massively parallel GPU-based simulation and optimized to work on a single workstation. 
                                We demonstrate the capability of scaling up Q-learning methods to tens of thousands of parallel environments.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="https://openreview.net/pdf?id=LVum7knUA7g" target="_blank">
                                <img src="files/gradient.png" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Stationary Deep Reinforcement Learning with Quantum K-spin Hamiltonian Equation
                            </papertitle>
                            <br>
                            <a href="http://www.tensorlet.org/">Xiao-Yang Liu*</a>,
                            <strong>Zechu Li*</strong>,
                            Shixun Wu,
                            <a href="https://www.ee.columbia.edu/~wangx/">Xiaodong Wang</a>
                            <br>
                            <em>Workshop on Physics for Machine Learning, International Conference on Learning Representations (ICLR) </em>, 2023
                            <br>
                            <a href="https://openreview.net/pdf?id=LVum7knUA7g" target="_blank">paper</a>
                            <p></p>
                            <p>
                                We propose a quantum K-spin Hamiltonian regularization term (called H-term) to help a policy network converge to a high-quality local minima. 
                                We take a quantum perspective by modeling a policy as a K-spin Ising model and employ a Hamiltonian equation to measure the energy of a policy.
                                We derive a novel Hamiltonian policy gradient theorem and design a generic actor-critic algorithm that utilizes the H-term to regularize the policy network. 
                                The proposed method significantly reduces the variance of cumulative rewards by 65.2% ~ 85.6% on six MuJoCo tasks, etc.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/4f550cb7b30b59553e50cd08a9dbf068-Abstract-Conference.html" target="_blank">
                                <img src="files/distributed_scheme.png" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Homomorphic Matrix Completion
                            </papertitle>
                            <br>
                            <a href="http://www.tensorlet.org/">Xiao-Yang Liu*</a>,
                            <strong>Zechu Li*</strong>,
                            <a href="https://www.ee.columbia.edu/~wangx/">Xiaodong Wang</a>
                            <br>
                            <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 2022
                            <br>
                            <a href="https://proceedings.neurips.cc/paper_files/paper/2022/hash/4f550cb7b30b59553e50cd08a9dbf068-Abstract-Conference.html" target="_blank">paper</a>
                            <p></p>
                            <p>
                                We propose a homomorphic matrix completion algorithm for privacy-preserving purpose. 
                                We first formulate a homomorphic matrix completion problem where a server performs matrix completion on cyphertexts, and propose an encryption scheme that is fast and easy to implement. 
                                We prove that the proposed scheme satisfies the homomorphism property and satisfies the differential privacy property. 
                                While with similar level of privacy guarantee, we reduce the best-known error bound to EXACT recovery at a price of more samples.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="https://ieeexplore.ieee.org/document/9827289" target="_blank">
                                <img src="images/social_learning.png" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>Social Learning In Markov Games: Empowering Autonomous Driving
                            </papertitle>
                            <br>
                            Xu Chen,
                            <strong>Zechu Li</strong>,
                            <a href="https://sharondi-columbia.wixsite.com/ditectlab">Sharon (Xuan) Di</a>
                            <br>
                            <em>IEEE Intelligent Vehicles Symposium (IV)</em>, 2022
                            <br>
                            <a href="https://ieeexplore.ieee.org/document/9827289" target="_blank">paper</a> /
                            <a href="https://github.com/supersglzc/Social-Learning">code</a>
                            <p></p>
                            <p>
                                We apply the social learning scheme to Markov games and leverage deep reinforcement learning (DRL) to investigate how individual AVs learn policies and form social norms in traffic scenarios. 
                                To capture agents' different attitudes toward traffic environments, a heterogeneous agent pool with cooperative and defective AVs is introduced to the social learning scheme.  
                                To solve social norms formed by AVs, we propose a DRL algorithm, and apply them to traffic scenarios: unsignalized intersection and highway platoon.
                            </p>
                        </td>
                    </tr>
                </table>
                
                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <a href="https://dl.acm.org/doi/10.1145/3490354.3494413" target="_blank">
                            <img src="images/finrl_podracer.png" alt="sym" width="100%" style="border-radius:5px">
                        </a>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>FinRL-Podracer: High Performance and Scalable Deep Reinforcement Learning for Quantitative Finance
                        </papertitle>
                        <br>
                        <strong>Zechu Li</strong>,
                        <a href="http://www.tensorlet.org/">Xiao-Yang Liu</a>,
                        Jiahao Zheng,
                        <a href="https://zhaoranwang.github.io/">Zhaoran Wang</a>, <br>
                        <a href="https://scholar.google.com.au/citations?user=Q5oC62EAAAAJ&hl=en">Anwar Walid</a>,
                        <a href="https://idea.edu.cn/person/guojian/">Jian Guo</a>
                        <br>
                        <em>ACM International Conference on AI in Finance (ICAIF)</em>, 2021
                        <br>
                        <a href="https://dl.acm.org/doi/10.1145/3490354.3494413" target="_blank">paper</a> /
                        <a href="https://github.com/AI4Finance-Foundation/FinRL_Podracer">code</a>
                        <p></p>
                        <p>
                            We introduce an RLOps in finance paradigm and present a FinRL-Podracer framework to accelerate the development pipeline of deep reinforcement learning (DRL)-driven trading strategy and to improve both trading performance and training efficiency.
                            We evaluate the FinRL-Podracer framework for a stock trend prediction task on an NVIDIA DGX SuperPOD cloud, and show the high scalability by training a trading agent in 10 minutes with 80 A100 GPUs, on NASDAQ-100 constituent stocks with minute-level data over 10 years.
                        </p>
                    </td>
                </tr>
            </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tr>
                      <td style="padding:20px;width:25%;vertical-align:middle">
                          <a href="https://arxiv.org/pdf/2112.05923.pdf" target="_blank">
                              <img src="images/erl_podracer.png" alt="sym" width="100%" style="border-radius:5px">
                          </a>
                      </td>
                      <td style="padding:20px;width:75%;vertical-align:middle">
                          <papertitle>ElegantRL-Podracer: Scalable and Elastic Library for Cloud-native Deep Reinforcement Learning
                          </papertitle>
                          <br>
                          <a href="http://www.tensorlet.org/">Xiao-Yang Liu*</a>,
                          <strong>Zechu Li*</strong>,
                          <a href="https://www.princeton.edu/~zy6/">Zhuoran Yang</a>,
                          Jiahao Zheng,
                          <a href="https://zhaoranwang.github.io/">Zhaoran Wang</a>, <br>
                          <a href="https://scholar.google.com.au/citations?user=Q5oC62EAAAAJ&hl=en">Anwar Walid</a>,
                          <a href="https://idea.edu.cn/person/guojian/">Jian Guo</a>,
                          <a href="http://people.eecs.berkeley.edu/~jordan/">Michael Jordan</a>
                          <br>
                          <em>Deep Reinforcement Learning Workshop, NeurIPS</em>, 2021
                          <br>
                          <a href="https://arxiv.org/pdf/2112.05923.pdf" target="_blank">paper</a> /
                          <a href="https://github.com/AI4Finance-Foundation/ElegantRL">code</a>
                          <p></p>
                          <p>
                            We present a scalable and elastic library ElegantRL-podracer for cloud-native deep reinforcement learning, which efficiently supports millions of GPU cores to carry out massively parallel training at multiple levels. 
                            At a high-level, ElegantRL-podracer employs a tournament-based ensemble scheme to orchestrate the training process on hundreds or even thousands of GPUs, scheduling the interactions between a leaderboard and a training pool with hundreds of pods. 
                            At a low-level, each pod simulates agent-environment interactions in parallel by fully utilizing nearly 7,000 GPU CUDA cores in a single GPU. 
                          </p>
                      </td>
                  </tr>
              </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                  <tr>
                      <td>
                          <heading>Open Source Projects</heading>
                      </td>
                  </tr>
              </table>

            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                  <td style="padding:20px;width:25%;vertical-align:middle">
                      <a href="https://github.com/AI4Finance-Foundation/FinRL" target="_blank">
                          <img src="images/finrl.png" alt="sym" width="100%" style="border-radius:5px">
                      </a>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:middle">
                      <papertitle>FinRL: Financial Reinforcement Learning
                      </papertitle>
                      <br>
                      <a href="https://finrl.readthedocs.io/en/latest/index.html" target="_blank">project page</a> /
                      <a href="https://github.com/AI4Finance-Foundation/FinRL">code</a> /
                      <a class="github-button" href="https://github.com/AI4Finance-Foundation/FinRL" data-icon="octicon-star" data-show-count="true" aria-label="Star AI4Finance-Foundation/FinRL on GitHub" target="_blank">GitHub Star</a>
                      <p></p>
                      <p>
                        FinRL is the first open-source framework to show the great potential of financial reinforcement learning. 
                        It has evolving into an ecosystem, including hundreds of financial markets, state-of-the-art algorithms, 
                        financial applications (portfolio allocation, cryptocurrency trading, high-frequency trading), live trading, cloud deployment, etc.
                      </p>
                  </td>
              </tr>
          </table>
                
             <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                <tr>
                    <td style="padding:20px;width:25%;vertical-align:middle">
                        <a href="https://github.com/AI4Finance-Foundation/ElegantRL" target="_blank">
                            <img src="images/elegantrl.jpg" alt="sym" width="100%" style="border-radius:5px">
                        </a>
                    </td>
                    <td style="padding:20px;width:75%;vertical-align:middle">
                        <papertitle>ElegantRL “小雅”: Massively Parallel Library for Cloud-native Deep Reinforcement Learning
                        </papertitle>
                        <br>
                        <a href="https://elegantrl.readthedocs.io/en/latest/index.html" target="_blank">project page</a> /
                        <a href="https://github.com/AI4Finance-Foundation/ElegantRL">code</a> /
                        <a class="github-button" href="https://github.com/AI4Finance-Foundation/ElegantRL" data-icon="octicon-star" data-show-count="true" aria-label="Star AI4Finance-Foundation/ElegantRL on GitHub" target="_blank">GitHub Star</a>
                        <p></p>
                        <p>
                            ElegantRL is a massively parallel library for cloud-native deep reinforcement learning (DRL) applications.
                            Our mission is to provide a scalable, efficienct, and accessible DRL platform for researchers and practitioners to develop cutting-edge RL applications. 
                            My specific interests are the development and usage of massively parallel simulations, and large-scale training with population-based training, ensemble methods, etc.
                            <br>
                            <br>
                            As a leader of this project, I have been contributing to 
                            <ul style="margin-top: -15px;">
                                <li>develop a series of large-scale training frameworks,</li>
                                <li>implemente SOTA algorithms and techniques,</li>
                                <li>build the documentation website.</li>
                            </ul>
                            Starting from Mar. 2021, I started to write tutorial blogs for the community,
                            <ul style="margin-top: 0px;">
                                <li><a href="https://medium.com/mlearning-ai/elegantrl-much-much-more-stable-than-stable-baseline3-f096533c26db">ElegantRL: Much More Stable Deep Reinforcement Learning Algorithms than Stable-Baseline3</a>, <br> <em>MLearning.ai</em>, Mar. 3, 2022.</li>
                                <li><a href="https://elegantrl.medium.com/elegantrl-podracer-scalable-and-elastic-library-for-cloud-native-deep-reinforcement-learning-bafda6f7fbe0">ElegantRL-Podracer: A Scalable and Elastic Library for Cloud-Native Deep Reinforcement Learning</a>, <br> <em>Towards data science</em>, Dec. 11, 2021.</li>
                                <li><a href="https://medium.com/@elegantrl/elegantrl-mastering-the-ppo-algorithm-part-i-9f36bc47b791">ElegantRL: Mastering PPO Algorithms</a>, <br> <em>Towards data science</em>, May. 3, 2021.</li>
                                <li><a href="https://medium.com/mlearning-ai/elegantrl-demo-stock-trading-using-ddpg-part-ii-d3d97e01999f">ElegantRL Demo: Stock Trading Using DDPG (Part II)</a>, <br> <em>MLearning.ai</em>, Apr. 19, 2021.</li>
                                <li><a href="https://elegantrl.medium.com/elegantrl-demo-stock-trading-using-ddpg-part-i-e77d7dc9d208">ElegantRL Demo: Stock Trading Using DDPG (Part I)</a>, <br> <em>MLearning.ai</em>, Mar. 28, 2021.</li>
                                <li><a href="https://towardsdatascience.com/elegantrl-a-lightweight-and-stable-deep-reinforcement-learning-library-95cef5f3460b">ElegantRL-Helloworld: A Lightweight and Stable Deep Reinforcement Learning Library</a>, <br> <em>Towards data science</em>, Mar. 4, 2021.</li>

                            </ul>

                        </p>
                    </td>
                </tr>
            </table>

                    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                      <tr>
                          <td>
                              <heading>Book Chapter</heading>
                          </td>
                      </tr>
                  </table>
                  
                  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tr>
                        <td style="padding:20px;width:25%;vertical-align:middle">
                            <a href="https://www.sciencedirect.com/science/article/pii/B9780128244470000157" target="_blank">
                                <img src="images/book.jpg" alt="sym" width="100%" style="border-radius:5px">
                            </a>
                        </td>
                        <td style="padding:20px;width:75%;vertical-align:middle">
                            <papertitle>High-performance Tensor Decompositions for Compressing and Accelerating Deep Neural Networks
                            </papertitle>
                            <br>
                            <a href="http://www.tensorlet.org/">Xiao-Yang Liu</a>,
                            Yiming Fang,
                            Liuqing Yang,
                            <strong>Zechu Li</strong>,
                            <a href="http://www.bell-labs.com/about/researcher-profiles/anwarwalid/#gref">Anwar Walid</a>
                            <br>
                            <em>Tensors for Data Processing, Elsevier</em>, 2021
                            <br>
                            <a href="https://www.sciencedirect.com/science/article/pii/B9780128244470000157" target="_blank">chapter</a> /
                            <a href="https://www.elsevier.com/books/tensors-for-data-processing/liu/978-0-12-824447-0">book</a>
                            <p></p>
                            <p>
                                Large-scale deep neural networks (DNNs) have led to impressive successes in many  applications.  
                                However, two challenges often arise in DNN deployment in Internet of Things (IoT) devices and real-time applications: training time and memory footprint.  
                                This chapter takes a practical approach to seek a better efficiency-accuracy trade-off, which utilizes high performance tensor decompositions to compress and accelerate neural networks by exploiting low-rank structures of the network weight matrix.
                            </p>
                        </td>
                    </tr>
                </table>

                <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
                    <tbody>
                        <tr>
                            <td>
                                <br>
                                <p align="right">
                                    <font size="2">
                                        <a href="https://people.eecs.berkeley.edu/~barron/" target="_blank">This guy makes a nice webpage.</a>
                                    </font>
                                </p>
                            </td>
                        </tr>
                    </tbody>
                </table>
                <!-- Global site tag (gtag.js) - Google Analytics -->
                <script async src="https://www.googletagmanager.com/gtag/js?id=UA-79592980-2"></script>
                <script>
                    window.dataLayer = window.dataLayer || [];

                    function gtag() {
                        dataLayer.push(arguments);
                    }
                    gtag('js', new Date());

                    gtag('config', 'UA-79592980-2');
                </script>

            </td>
        </tr>
    </table>
</body>

</html>
